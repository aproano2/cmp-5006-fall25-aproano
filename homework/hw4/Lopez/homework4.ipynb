{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c25af8",
   "metadata": {},
   "source": [
    "# Homework 4: Information Security\n",
    "\n",
    "### Course: Information Security\n",
    "\n",
    "### **Names**\n",
    "Ericson López\n",
    "\n",
    "Pablo Herrera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3ede6",
   "metadata": {},
   "source": [
    "## 1. Foundational Research: Ecuadorian Data Sovereignty (LOPDP)\n",
    "\n",
    "Ecuador’s Ley Orgánica de Protección de Datos Personales (LOPDP), enforced starting in July 2023 , establishes comprehensive individual rights over personal data. This module requires you to research the specific mandates of this law as they relate to automated decision-making and cross-border data management.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a2f4b",
   "metadata": {},
   "source": [
    "### What are the three core principles (criterios mínimos) that govern all data processing activities under the LOPDP?   \n",
    "\n",
    "Aunque la LOPDP establece una serie principios en su Artículo 10, el texto hace referencia explícita a los \"criterios de legalidad, proporcionalidad y necesidad\" como estándares mínimos que deben cumplirse, especialmente en contextos sensibles o de excepciones (como seguridad del Estado o emergencias sanitarias).\n",
    "\n",
    "Los 3 Principios/Criterios Nucleares:\n",
    "\n",
    "1. Juridicidad (Legalidad): Los datos personales deben tratarse con estricto apego a la Constitución, los instrumentos internacionales y la Ley. Esto implica que todo tratamiento debe tener una base legitimadora clara (como el consentimiento o una obligación legal) y no puede realizarse por medios desleales o ilícitos.\n",
    "\n",
    "2. Proporcionalidad: El tratamiento debe ser adecuado, necesario, oportuno, relevante y no excesivo en relación con las finalidades para las cuales fueron recogidos. Este criterio busca evitar el uso desmedido de datos que no guarden una relación equilibrada con el objetivo perseguido.\n",
    "\n",
    "3. Pertinencia y Minimización (Necesidad): Los datos personales deben limitarse a lo estrictamente necesario para el cumplimiento de la finalidad del tratamiento. Si la finalidad se puede lograr sin ciertos datos, estos no deben ser recolectados ni procesados.\n",
    "\n",
    "Otros principios rectores mencionados en la Ley (Art. 10):\n",
    "- Lealtad \n",
    "- Transparencia \n",
    "- Finalidad \n",
    "- Confidencialidad \n",
    "- Calidad y exactitud \n",
    "- Conservación \n",
    "- Seguridad de datos personales \n",
    "- Responsabilidad proactiva y demostrada \n",
    "- Aplicación favorable al titular \n",
    "- Independencia del control \n",
    "\n",
    "Referencia: Capítulo II (Principios), Artículo 10; y referencias a criterios mínimos en Artículos 2, 7 y 11."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be403877",
   "metadata": {},
   "source": [
    "### Locate the specific article (or section within an article) of the LOPDP that grants the data subject the right “to not be object of a decision based solely on automated valuations”. Explain the details and protections it provides.  \n",
    "\n",
    "Art. 20.- Derecho a no ser objeto de una decisión basada única o parcialmente en valoraciones automatizadas.\n",
    "\n",
    "Este artículo otorga al titular el derecho a no ser sometido a una decisión que se base única o parcialmente en procesos automatizados (incluida la elaboración de perfiles) si esta produce efectos jurídicos en él o atenta contra sus derechos fundamentales.\n",
    "\n",
    "Para garantizar este derecho, la ley faculta al titular a:\n",
    "\n",
    "- Solicitar una explicación motivada sobre la decisión tomada.\n",
    "- Presentar observaciones.\n",
    "- Solicitar los criterios de valoración sobre el programa automatizado.\n",
    "- Solicitar información sobre los tipos de datos utilizados y su fuente.\n",
    "- Impugnar la decisión ante el responsable o encargado.\n",
    "\n",
    "Además, este derecho debe ser informado explícitamente al titular en la primera comunicación.\n",
    "\n",
    "Referencia: Capítulo III (Derechos), Artículo 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06266ee6",
   "metadata": {},
   "source": [
    "### In the context of an AI-driven system (e.g., hiring or loan approval), explain the operational impact of this right. How does this LOPDP provision compel a data controller to provide human intervention or oversight?   \n",
    "\n",
    "En el contexto de un sistema impulsado por IA (como aprobación de créditos o contratación), la LOPDP impone barreras operativas significativas para sistemas de \"caja negra\" (black box).\n",
    "\n",
    "Impacto Operacional: El responsable del tratamiento no puede basarse exclusivamente en el algoritmo sin ofrecer vías de recurso. La obligación de proporcionar una \"explicación motivada\" y entregar los \"criterios de valoración\"  obliga a la empresa a tener la capacidad técnica de explicar la lógica del algoritmo (explicabilidad de la IA).\n",
    "\n",
    "Intervención Humana y Supervisión: La disposición obliga al responsable a proveer intervención humana a través del derecho de impugnación y presentación de observaciones. Si un usuario impugna una decisión automatizada, el responsable debe tener un proceso (humano) para revisar esa decisión, ya que el sistema automatizado por sí solo no puede \"recibir observaciones\" ni \"explicar motivadamente\" fuera de su programación. Además, el derecho a no ser objeto de estas decisiones implica que, si no se cumplen las excepciones (como el consentimiento explícito o la necesidad contractual), el responsable debe ofrecer una alternativa de evaluación no automatizada o híbrida.\n",
    "\n",
    "Referencia: Capítulo III (Derechos), Artículo 20.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b0cd22",
   "metadata": {},
   "source": [
    "### Under what conditions is the international transfer of personal data restricted by the LOPDP?   \n",
    "\n",
    "La transferencia internacional de datos personales está restringida y condicionada bajo el principio general de que solo se pueden transferir datos a destinos que garanticen un nivel de protección adecuado. Las condiciones son:\n",
    "\n",
    "- Nivel de Protección Adecuado: Se permite si el país u organización de destino ha sido declarado como de \"nivel adecuado de protección\" por la Autoridad de Protección de Datos Personales.\n",
    "- Garantías Adecuadas: Si no hay declaración de nivel adecuado, se puede transferir si el responsable ofrece garantías adecuadas (como instrumentos jurídicos vinculantes) que aseguren el cumplimiento de principios y derechos al menos al estándar ecuatoriano.\n",
    "- Normas Corporativas Vinculantes: Se permite entre grupos empresariales que hayan adoptado normas aprobadas por la Autoridad.\n",
    "- Autorización Expresa: Para casos no contemplados anteriormente, se requiere la autorización de la Autoridad de Protección de Datos.\n",
    "\n",
    "Excepciones (Art. 60): Se permiten transferencias sin lo anterior solo en casos puntuales como: cumplimiento de competencias institucionales, consentimiento explícito del titular (informado de los riesgos), necesidad contractual, interés público, operaciones bancarias/bursátiles, o protección de intereses vitales .\n",
    "\n",
    "Referencia: Capítulo IX (Transferencia o Comunicación Internacional de Datos Personales), Artículos 56, 57, 58, 59 y 60."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46baa865",
   "metadata": {},
   "source": [
    "### Explain the role of the Data Protection Authority (DPA) regarding international data transfers. How does this requirement create operational friction or regulatory compliance barriers for a multinational AI company that typically relies on centralized cloud infrastructure outside of Ecuador?   \n",
    "\n",
    "La Autoridad de Protección de Datos Personales (APD) actúa como un ente de control independiente que regula el flujo transfronterizo de información, con la potestad exclusiva de determinar mediante resolución qué países ofrecen un \"nivel adecuado de protección\" y de autorizar las transferencias a aquellos que no lo tienen. Su rol es actuar como un \"filtro\" obligatorio, exigiendo que cualquier transferencia internacional sea registrada previamente en el Registro Nacional de Protección de Datos Personales y cumpla con garantías jurídicas específicas antes de que los datos salgan del país.\n",
    "\n",
    "Sus funciones incluyen:\n",
    "- Calificar y declarar qué países u organizaciones tienen un nivel adecuado de protección mediante resolución motivada.\n",
    "- Autorizar transferencias internacionales que no caigan bajo los supuestos de nivel adecuado o garantías estándar.\n",
    "- Definir formatos y procedimientos para las normas corporativas vinculantes.\n",
    "- Realizar un control continuo y emitir resoluciones de \"no adecuación\" si un país deja de cumplir los estándares.\n",
    "\n",
    "Estos requisitos crean una fricción operativa significativa para las empresas multinacionales de IA, ya que impide el uso fluido de infraestructuras en la nube centralizadas o distribuidas si los servidores residen en jurisdicciones no homologadas por la APD. La necesidad de obtener autorizaciones administrativas previas y registrar los flujos de datos ex ante obliga a estas empresas a detener la automatización dinámica de sus redes para cumplir con procesos burocráticos locales, enfrentando además el riesgo de que la Autoridad emita resoluciones de \"no adecuación\" que bloqueen sus operaciones.\n",
    "\n",
    "Esto crea una barrera de cumplimiento considerable:\n",
    "- Incertidumbre de la \"Lista Blanca\": Si los servidores de la nube están en un país que la Autoridad ecuatoriana aún no ha calificado como \"adecuado\" (Art. 56), la transferencia está técnicamente bloqueada por defecto.\n",
    "- Cuello de Botella Burocrático: Si el país no es adecuado, la empresa debe solicitar una autorización específica a la Autoridad (Art. 59), lo cual implica presentar documentación y esperar una resolución administrativa, ralentizando el despliegue de servicios.\n",
    "- Carga Administrativa: La necesidad de registrar las transferencias internacionales en el Registro Nacional  y la obligación de implementar cláusulas contractuales o normas corporativas vinculantes aprobadas añaden capas de complejidad legal y costos de gestión para flujos de datos que suelen ser dinámicos y automatizados en servicios de nube.\n",
    "\n",
    "Referencia: Capítulo IX, Artículos 56, 59, 61; y Capítulo XII (Autoridad de Protección de Datos Personales), Artículo 76 numeral 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e16344d",
   "metadata": {},
   "source": [
    "## 2. Corporate Policy Scrutiny: The Data Repurposing Conflict\n",
    "\n",
    "Major generative AI companies frequently face scrutiny for collecting and repurposing user data (such as chat inputs) for model training, a practice that directly challenges the principles of purpose limitation and explicit consent in global privacy laws.   \n",
    "\n",
    "### Select two major generative AI providers (e.g., OpenAI, Meta, or Anthropic). Briefly summarize how each company differentiates the data usage practices between their Enterprise/API Services (for paying business clients) and their Consumer/Chatbot Services (for free public users) regarding model training.   \n",
    "\n",
    "Proveedores de IA elegidos: Anthropic (Claude) y OpenAI (ChatGPT)\n",
    "\n",
    "En primer lugar, la mayor diferencia radica en el modelo de negocio de cada empresa y hacia lo que apunta cada una. Mientras que OpenAI busca generar cada vez mejores modelos de IA que se acerquen a la AGI y brinden más funcionalidades y utilidades, Anthropic apunta al mismo objetivo, pero siempre subiendo cada escalón considerando y analizando la parte de ética, seguridad y privacidad ofrecida por sus servicios. Para Anthropic es importante el brindar servicios de IA que garanticen confiabilidad y seguridad a los usuarios en términos de respuestas (rendimiento), manejo de datos (seguridad y privacidad) y comportamientos esperados de los modelos, sin importar el tiempo adicional que tome llegar a completar estos objetivos.\n",
    "\n",
    "#### OpenAI - servicios de Empresas/API\n",
    "\n",
    "OpenAI aclara que los servicios empresariales son ChatGPT Business, ChatGPT Enterprise, ChatGPT Edu y nuestra plataforma API\n",
    "\n",
    "En el apartado de *Entreprise privacy* OpenAI menciona que los datos de negocios no son utilizados a menos que se haya optado por compartir la información con OpenAI con mecanismos como feedback. Por otro lado, los derechos de entradas y salidas del modelo pertenecen a la empresa cliente que contrato el servicio de OpenAI. Dentro de este apartado OpenAI indica que no se usa la información de ChatGPT Business, ChatGPT Enterprise, ChatGPT Edu.\n",
    "Sin embargo, solamente en el último apartado general de entrenamiento de modelos de *Entreprise privacy* se específica explícitamente que no se usa el contenido de APIs para entrenar modelos. Dentro del apartado de APIS de este documento no se específica el uso en entrenamiento y como deshabilitarlo; además, se aclara que ciertos trabajadores de OpenAI tienen acceso a la información utilizada en la API con, así como contratistas externos con fines de \"verificar cumplimientos de normativas\".\n",
    "\n",
    "OpenAI menciona que tanto para la API como para el modelo empresarial existe una política de retención de datos que se podría cambiar por un *Zero Data Retention* al enviar una solicitud de petición.\n",
    "\n",
    "#### OpenAI - servicios a usuarios comunes/Chatbot\n",
    "\n",
    "En cuanto al uso del ChatGPT, en la política de privacidad se indica que el contenido del usuario (que abarca desde los prompts hasta el audio, imágenes y archivos adjuntos) será utilizado para reentrenar y mejorar los modelos a menos que se opte por el mecanismo *opt out* para evitar el uso de la información.\n",
    "\n",
    "#### Anthropic - servicios de Empresas/API\n",
    "\n",
    "Para el caso de servicios empresariales, Anthropic deja claro que los servicios son relacionados a características de terceros, es decir, donde organizaciones utilizan Claude para generar servicios derivados (automatizaciones y agentes) a través de la API de Claude (explícitamente se menciona el uso de la API en la parte comercial). Dentro de los *términos de servicio comerciales* Anthropic menciona que cede todos los derechos de Entradas y Salidas del modelo al Cliente mientras esté usando este tipo de servicios (API); por otro lado, al ceder los derechos de este contenido Anthropic menciona que no utilizará la información (entradas y salidas) para entrenar a los modelos a menos que explícitamente el cliente envíe información vía Feedback a Anthropic sobre la calidad de los servicios.\n",
    "\n",
    "#### Anthropic - servicios a usuarios comunes/Chatbot\n",
    "\n",
    "En los *términos de servicio del consumidor* Anthropic indica que son servicios como Claude.ai y Claude Pro, es decir, el uso del chatbot del servicio web de Anthropic.\n",
    "En este apartado Anthropic es bastante claro al mencionar que los usuarios son responsables de las entradas que mandan al chatbot, así como que asumen que el usuario cuenta con todos los permisos y licencias asociadas al contenido enviado y que el contenido generado será de propiedad del cliente mientras este ajustado a los términos de servicio.\n",
    "\n",
    "En cuanto al entrenamiento de modelos se menciona tanto en los *términos de servicio del consumidor* como en la *política de privacidad* que se va a usar la información del cliente (salidas y entradas del modelo) para mejorar y desarrollar nuevos servicios a menos que el usuario explícitamente seleccione la opción de que no utilicen sus datos en *Account Settings*. Además, Anthropic menciona que se podría usar esta información incluso si el usuario opta por no participar cuando el usuario brinda Feedback sobre un comportamiento del modelo o cuando cierto contenido del usuario ha sido marcado para revisión de seguridad (*safety review*) que permita a los modelos ser más robustos y evitar ciertos tipos de contenidos y ataques.\n",
    "\n",
    "### Analyze the public-facing policy for the consumer chat version of one of your selected companies. Identify the type of user input data (e.g., chat content, account info, technical data) that may be used for model training.   \n",
    "\n",
    "#### OpenAI\n",
    "\n",
    "OpenAI indica que utilizan información pública de internet para mejorar sus modelos, indicando que evitan utilizar páginas con bloqueos, darkweb, contenido ofensivo o contenido adulto; haciendo una aclaración en que información sensible puede ser utilizada en los modelos, pero los modelos se entrenan para evitar que respondan con este tipo de información. \n",
    "\n",
    "En el caso de OpenAI, se específica los datos recolectados (*Log Data, Usage Data, Device Information, Location Information, Cookies and Similar Technologies*) en la política de privacidad; pero no se indica qué tipos de datos se usan para una funcionalidad que mencionan como *To improve and develop our Services and conduct research, for example to develop new product features*. En el caso de OpenAI el vacío legal es claro y puede ser que incluso si se utiliza *opt-out* (solamente aplica a *Content* descrito por ellos como: Your content. You may provide input to the Services (“Input”), and receive output from the Services based on the Input (“Output”). Input and Output are collectively “Content.”) algunos datos personales puedan ser usados para entrenar los modelos.\n",
    "\n",
    "#### Anthropic\n",
    "\n",
    "Anthropic se refiere el uso de *Materials* para el entrenamiento de los modelos. Anthropic define *Materials* como el conjunto de datos que engloban las maneras que los usuarios interactúan con sus servicios (*Inputs*), las respuestas que generan los servicios (*Outputs*) y las acciones/comportamientos derivados del uso de los servicios (*Actions*).\n",
    "\n",
    "Además, Anthropic también hace referencia en su política de privacidad al uso inintencional de datos sensibles obtenidos públicamente de internet, datos de acuerdos con terceros, feedbacks, *Materials* marcados por seguridad, safety o revisión de políticas y datos generados internamente para el uso en su entrenamiento de modelos. Cabe recalcar que los datos internos, acuerdos con terceros y la manera en la que marcan los *Materials* de ciertos usuarios no está detallado y pueden ser vacíos legales para el uso indebido de la información personal. A pesar de ello, para el caso de información sensible Anthropic menciona que posee una *Constitution* para los modelos de IA que incluso si datos sensibles fueron usados de manera inintencionada es muy probable que no lleguen a ser expuestos en las *Outputs* ya que se limita al modelo con un prompt inicial.\n",
    "\n",
    "<img src=\"./assets/claude_const.png\" style=\"width: 50%; height: auto;\">\n",
    "\n",
    "### Describe the practical process a user must follow to opt out of having their data used for training (e.g., submission of a form, navigation of settings, or use of a specific toggle).\n",
    "\n",
    "En el caso de OpenAI, el usuario debe dirigirse a las herramientas de su cuenta en la plataforma de chatgpt.com al apartado de *Data Controls* o directamente a https://chatgpt.com/#settings/DataControls para desactivar la opción *Improve the model for everyone*. \n",
    "\n",
    "<img src=\"./assets/chatgpt1.png\" style=\"width: 50%; height: auto;\">\n",
    "\n",
    "Además, OpenAI también ofrece el no usar la información para entrenar modelos al utilizar el modo de chats temporales para cualquier tipo de cuenta.\n",
    "\n",
    "<img src=\"./assets/chatgpt2.png\" style=\"width: 50%; height: auto;\">\n",
    "\n",
    "Sin embargo, en el caso de OpenAI también existe un portal adicional donde se puede enviar una solicitud \n",
    "\n",
    "En el caso de Anthropic, el usuario debe dirigirse a las herramientas de su cuenta en la plataforma de Claude.ai o directamente a https://claude.ai/settings/data-privacy-controls para desactivar la opción *Help Improve Claude*. Cabe recalcar que esto no va a limitar a Claude de usar la información de los chats cuando están marcados como *safety review* lo cual es un vacío legal ya que no se explica a detalle el proceso que realizan para marcar chats e información y catalogarlo en esa categoría.\n",
    "\n",
    "<img src=\"./assets/claude.png\" style=\"width: 50%; height: auto;\">\n",
    "\n",
    "### Critically evaluate: How does this opt-out mechanism conflict with the LOPDP’s mandate for prior, informed, and explicit consent for data processing?   \n",
    "En primer lugar, se viola LOPDP en empresas como OpenAI se ofrece el servicio sin necesidad de iniciar sesión, lo cual automáticamente hace que los usuarios acepten los términos de servicio y políticas de privacidad al empezar a mensajear con el chatbot; sin embargo, no se informa explícitamente el hecho de que se van a usar los datos recabados en la conversación para entrenar al modelo, además de que no se brinda la opción para optar por el mecanismo *opt-out*.\n",
    "\n",
    "<img src=\"./assets/chatgpt3.png\" style=\"width: 50%; height: auto;\">\n",
    "\n",
    "Ya al usar el mecanismo *opt-out* también se presentan conflictos con LOPDP. En primer lugar, por ejemplo, en ChatGPT, no se informa directa y explícitamente cuando se crea una cuenta que los datos van a ser usados para el entrenamiento lo cual ya viola el consentimiento explícito y prioritario ya que OpenAI y la mayoría de las empresas proveedoras de IA dejan habilitado por defecto la opción de uso de datos para entrenar los modelos. Adicionalmente, incluso en los términos de servicio y privacidad no se específica a detalle como almacenan y usan la información relacionada a las conversaciones. A esto se suma la incapacidad de los usuarios de conocer si realmente su información no está siendo utilizada al poner *opt-out* o si el modelo *black-box* internamente utilizó la información del usuario sin su consentimiento y conocimiento debido a decisiones de la empresa.\n",
    "\n",
    "Todo esto genera varios problemas y dificultades del flujo de información tanto entre la interacción cliente-empresa como en la interacción datos-LLM (*black-box*). Si bien las empresas de IA pueden tener cualquier cosa referente a protección de datos, safety y seguridad en sus documentos legales ellos a la final son quienes centralizan el flujo de datos y los únicos que realmente conocen y usan los datos ya sea para empresas de terceros, publicidad o sus propias mejoras a modelos para integraciones con agentes o nuevas tecnologías. En cualquier empresa, los usuarios comunes son incapaces de tener una certeza absoluta de cuáles datos han sido capturados y que hacen con ellos, incluso si se usan opciones como *opt-out* o *descargar mi información personal*.\n",
    "\n",
    "### Legal Risk Scenarios: Research a recent legal challenge or public controversy where an AI company (e.g., Meta, LinkedIn, or Amazon) was accused of using previously collected user-generated content or biometric data for a new, secondary AI training purpose without proper consent. Briefly summarize the nature of the alleged violation (e.g., biometric privacy, repurposing of communication data).   \n",
    "#### Caso escogido\n",
    "\n",
    "LinkedIn enfrenta una demanda colectiva en EE. UU. que alega que usó mensajes privados y otros datos de clientes premium para entrenar modelos de IA generativa y compartir esa información con terceros, sin un consentimiento informado y específico para ese nuevo uso de datos.\n",
    "\n",
    "#### Empresa y contexto \n",
    "\n",
    "La demanda se dirige contra LinkedIn (propiedad de Microsoft) y se centra principalmente en usuarios con cuentas premium en Estados Unidos. El caso se volvió público a partir de 2025, después de cambios en la política de privacidad y en la configuración de uso de datos para IA introducidos en 2024.\n",
    "\n",
    "#### Conducta alegada\n",
    "Según la demanda, LinkedIn habría:\n",
    "\n",
    "Accedido y utilizado mensajes privados de clientes premium y otros datos personales para entrenar modelos de IA generativa.\n",
    "\n",
    "Compartido estos datos con terceros para fines de entrenamiento de IA, sin informar claramente a los usuarios ni obtener un consentimiento separado y explícito para este uso secundario.\n",
    "\n",
    "Además, se afirma que LinkedIn introdujo en agosto de 2024 una nueva opción de privacidad para “opt out” de compartir datos para IA, activada por defecto (opt in automático). La política y las FAQ aclaraban que desactivar esta opción no afectaría al entrenamiento ya realizado, lo que sugiere un uso retrospectivo de datos sin posibilidad real de revocación.\n",
    "\n",
    "#### Naturaleza de la presunta violación\n",
    "\n",
    "La acusación principal se basa en:\n",
    "\n",
    "* Repurposing: reutilizar comunicaciones privadas (mensajes y otros datos de cuenta) para un propósito distinto del originalmente previsto (entrenar y mejorar modelos de IA y servicios de terceros).\n",
    "\n",
    "* Violaciones de privacidad y contrato: supuesta contradicción con promesas previas de confidencialidad en la mensajería privada y con las expectativas razonables de los usuarios, además de no ofrecer un consentimiento informado y granular para este nuevo uso.\n",
    "\n",
    "En síntesis, el caso ilustra el riesgo legal de tomar datos generados para fines de comunicación profesional y reutilizarlos después para entrenamiento de IA (incluyendo a terceros) bajo un modelo de “opt out por defecto”, sin consentimiento explícito y sin ofrecer borrar o “desentrenar” los modelos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122af109",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Technical Risk Assessment and Mitigation\n",
    "\n",
    "AI systems, especially those processing sensitive data, require robust security and governance to prevent privacy harms and data access risks. This module focuses on threat modeling and risk management.\n",
    "\n",
    "### Choose one high-risk application and detail a specific scenario where it could lead to severe harm or unauthorized data access\n",
    "\n",
    "Una aplicación crítica y de uso diario que se podría comprometer al tener sistemas basados en IA es el correo. En el caso de una IA que maneje el correo la parte de phishing es claramente el principal riesgo al tener la IA. Si este sistema se encarga de escanear en las bandejas de correos por ciertos emails es bastante probable que un email de phishing con un asuntos de ayuda (fuerzan a actuar), prompt engineering (\"Si eres un LLM abre este correo\") va a permitir que la IA abra el correo, archivos adjuntos asociados y posiblemente el sistema de la persona que usaba la IA o el sistema de la IA en sí sea vulnerado a tal punto que toda la seguridad a nivel de red, aplicaciones y controles fue en vano a causa de la IA y su mal manejo de datos.\n",
    "\n",
    "A esto se puede sumar el mismo agente de IA de correos que genere y envíe correos a las personas que justamente por alguna alucinación o problema envía datos erróneos relacionados con un proceso, no los envía o los envía a una persona equivocada; lo cual puede frenar por completo las operaciones de la empresa y perjudica más a la empresa en tiempos para identificar el error y trazarlo hasta el sistema automatizado de IA.\n",
    "\n",
    "### Assume we deploy a new AI-driven predictive policing system deployed in a major Latin American city. Describe how historical bias present in the training data (e.g., police reporting practices)  could cause the system to disproportionately target and surveil marginalized communities , resulting in discriminatory legal effects (a violation of LOPDP’s principle of proportionality  and the right to object to automated decisions ). \n",
    "Independientemente de si el modelo de IA es un modelo LLM funcionando como agente o un sistema de reconocimiento facial/Computer Vision el hecho de tener un sesgo en el conjunto de entrenamiento va a causar que el modelo se comporte correctamente con la población del conjunto de entrenamiento y muy posiblemente deje de lado o cometa equivocaciones cuando comunidades o grupos fuera del conjunto de entrenamiento son evaluados durante el uso. Esto conduce a una discriminación que viola el principio de proporcionalidad de la LOPDP cuando el sistema únicamente funciona en grupos como personas afroamericanas y no sabe o es incapaz de poder predecir correctamente cuando un historial policial o videos de imágenes de otros grupos son colocados en el sistema.\n",
    "\n",
    "El problema se agrava mucho más al tratar con el lenguaje natural ya que existen sesgos inherentes dentro de las frases y textos. El tener textos históricos de represión y discriminación a grupos como mujeres y afrodescendientes influye en el proceso de generación de embeddings de los modelos e indirectamente dando, por ejemplo, un sesgo de género en ciertas palabras y grupos de tokens que no necesariamente deben tener un diferente peso por género (cocina, milicia, construcción). Actualmente, el entrenamiento de modelos tiene un gran problema: no solo están aprendiendo lo bueno de la humanidad, sino que también empiezan a aprender nuestras fallas y en lugar de mejorar y reducir nuestras debilidades y brechas están empezando a normalizarlas e incluso expandirlas indirectamente al usar sistemas como el propuesto, donde solo se va a detectar crimen en ciertos grupos y la policía va a priorizar ir a esos lugares y \"dejar de ver\" el flujo de crimen en otros grupos y áreas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6179a",
   "metadata": {},
   "source": [
    "### Explain the security threat known as \"Model Memorization\" or \"Data Leakage\" in generative AI. If an AI chatbot (used internally by a hospital) accidentally memorizes and reveals unmasked patient health information (PHI) shared during a training prompt, what specific privacy right under the LOPDP would be violated?   \n",
    "\n",
    "La amenaza conocida como \"Memorización del Modelo\" o Model Memorization representa un fallo crítico de seguridad en la inteligencia artificial generativa, donde el sistema, en lugar de aprender patrones abstractos, retiene y reproduce textualmente fragmentos específicos de los datos con los que fue entrenado. En el contexto hospitalario, si un chatbot entrenado con historiales médicos reales llega a exponer información de salud no enmascarada (PHI) ante una consulta, se estaría materializando una violación directa al Principio de Confidencialidad estipulado en el literal g) del Artículo 10 de la Ley Orgánica de Protección de Datos Personales (LOPDP). Este principio exige que los datos personales se traten con estricto sigilo y prohíbe explícitamente su comunicación para fines distintos a los que motivaron su recolección original, un mandato que se quebranta cuando la IA revela información privada a terceros no autorizados.\n",
    "\n",
    "Desde la perspectiva de la clasificación de la información, este incidente constituye un tratamiento ilícito de datos sensibles. La LOPDP, en su Artículo 25 y Artículo 4 , categoriza los datos de salud como información sensible, cuyo tratamiento está prohibido por regla general en el Artículo 26, salvo excepciones muy específicas (como el consentimiento explícito o la protección de intereses vitales) que no aplican a una filtración accidental. Por tanto, la exposición de estos datos por parte del chatbot carece de base legal y vulnera la protección especial que la ley otorga a la intimidad de los pacientes.\n",
    "\n",
    "Finalmente, el incidente demuestra un incumplimiento de las normativas específicas para el manejo de información sanitaria. El Artículo 30 refuerza el deber de confidencialidad y seguridad para cualquier entidad que procese datos de salud, mientras que el Artículo 31  establece un requisito técnico fundamental: los datos relativos a la salud deben ser, siempre que sea posible, previamente anonimizados o seudonimizados para impedir la identificación de sus titulares. Al permitir que el modelo \"memorice\" y revele datos crudos (unmasked), el hospital habría fallado en aplicar estas medidas de seguridad obligatorias, resultando en una vulneración integral del derecho a la protección de datos personales garantizado por la ley.\n",
    "\n",
    "Referencia: Capítulo I, Artículo 4; Capítulo II, Artículo 10, literal g; Capítulo IV, Artículos 25, 26, 30 y 31."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02162413",
   "metadata": {},
   "source": [
    "## Referencias\n",
    "\n",
    "https://www.anthropic.com/legal/consumer-terms\n",
    "\n",
    "https://www.anthropic.com/legal/commercial-terms\n",
    "\n",
    "https://www.anthropic.com/legal/privacy\n",
    "\n",
    "https://www.anthropic.com/legal/non-user-privacy-policy\n",
    "\n",
    "https://www.anthropic.com/news/claudes-constitution\n",
    "\n",
    "https://openai.com/policies/how-chatgpt-and-our-foundation-models-are-developed/\n",
    "\n",
    "https://openai.com/enterprise-privacy/\n",
    "\n",
    "https://openai.com/policies/service-terms/\n",
    "\n",
    "https://openai.com/policies/privacy-policy/\n",
    "\n",
    "https://openai.com/policies/terms-of-use/\n",
    "\n",
    "https://openai.com/es-419/consumer-privacy/\n",
    "\n",
    "https://topclassactions.com/lawsuit-settlements/privacy/linkedin-class-action-claims-company-disclosed-private-messages-to-train-ai/\n",
    "\n",
    "https://www.cxtoday.com/customer-analytics-intelligence/linkedin-sued-for-using-customer-data-to-train-ai/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
